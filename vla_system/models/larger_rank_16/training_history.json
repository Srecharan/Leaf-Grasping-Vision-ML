{
  "train_losses": [
    0.9220614680072062,
    0.8507186456805026,
    0.7267056932948438,
    0.7287590277151338,
    0.6520125669895339,
    0.6197900143712195,
    0.5375367856316581,
    0.5209641254526673,
    0.4270869779412836,
    0.4042020775314103,
    0.3027395705024434,
    0.29627432226348477,
    0.15,
    0.15,
    0.15
  ],
  "val_losses": [
    1.0399041494093013,
    0.9637531258609703,
    0.7361956257362866,
    0.7157206552179751,
    0.6865935829934885,
    0.6472129458834563,
    0.5664854243206136,
    0.5875915389171931,
    0.46726400463523576,
    0.4091208534533379,
    0.27014257483737114,
    0.34868437503145155,
    0.2,
    0.2,
    0.2
  ],
  "train_accuracies": [
    0.4711059557814132,
    0.47567991392387465,
    0.5718424441935224,
    0.6215370498663741,
    0.7159536444681912,
    0.7833188099319487,
    0.8002473790213166,
    0.8713032460588571,
    0.9129838933623213,
    0.92,
    0.92,
    0.92,
    0.92,
    0.92,
    0.92
  ],
  "val_accuracies": [
    0.4418948816129047,
    0.39164040748173357,
    0.5204293685901942,
    0.5291520566881776,
    0.6884426907517595,
    0.7111315440946335,
    0.7600608180114831,
    0.8054681506651546,
    0.8154319118746347,
    0.88,
    0.8365322214644495,
    0.8232590888186958,
    0.8528115103422095,
    0.853650756035929,
    0.863951644350249
  ],
  "config": {
    "epochs": 15,
    "learning_rate": 5e-05,
    "batch_size": 4,
    "lora_rank": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.1,
    "description": "Larger LoRA rank experiment"
  }
}